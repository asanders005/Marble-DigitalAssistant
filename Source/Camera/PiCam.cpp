#include "PiCam.h"

#include <core/buffer_sync.hpp>
#include <core/completed_request.hpp>
#include <core/frame_info.hpp>
#include <core/still_options.hpp>
#include <image/image.hpp>
#include <libcamera/camera_manager.h>

#include <atomic>
#include <fstream>
#include <iostream>
#include <memory>
#include <algorithm>
#include <cmath>
#include <iomanip>
#include <sstream>
#include <opencv2/imgcodecs.hpp>
#include <opencv2/imgproc.hpp>
#include <vector>

// Code generated by GitHub Copilot, edited by Aiden Sanders

void PiCam::ConfigureStill(unsigned int width, unsigned int height, int quality)
{
    // set values into the camera's options (OptsInternal) via Options::Set()
    camera.GetOptions()->Set().width = width;
    camera.GetOptions()->Set().height = height;
    camera.GetOptions()->Set().encoding = "jpg";
    camera.GetOptions()->Set().quality = quality;
    camera.GetOptions()->Set().raw = false;      // Explicitly disable raw output
    camera.GetOptions()->Set().nopreview = true; // disable preview thread for headless operation

    // If an explicit camera isn't set in options, pick the first available libcamera camera.
    try
    {
        libcamera::CameraManager cm;
        cm.start();
        auto cams = cm.cameras();
        if (cams.size() == 0)
        {
            cm.stop();
            throw std::runtime_error("No libcamera cameras found");
        }
        // Set the camera index in options so RPiCamApp opens the correct device (use first camera)
        camera.GetOptions()->Set().camera = 0;
        cm.stop();
    }
    catch (const std::exception &e)
    {
        std::cerr << "Warning: could not auto-select camera: " << e.what() << "\n";
    }

    // Make a couple of explicit option choices to avoid invalid defaults
    camera.GetOptions()->Set().denoise = std::string("off");
    camera.GetOptions()->Set().metering = std::string("average");
    camera.GetOptions()->Set().awb = std::string("auto");

    // Open and start the camera once here so repeated captures don't re-enumerate devices.
    try
    {
        camera.OpenCamera();
        // Request default processed images (allow pipeline to pick YUV420/sYCC as it does for
        // the OV5647). We will convert from YUV -> BGR in CaptureToMat; requesting BGR here
        // has produced mismatches on some pipelines.
        camera.ConfigureStill();
        camera.StartCamera();
        cameraRunning = true;
    }
    catch (const std::exception &e)
    {
        std::cerr << "Error opening/starting camera: " << e.what() << "\n";
        std::cerr << "Possible causes: requested resolution/format requires too large CMA buffers; "
                     "trying fallback resolution 640x480..."
                  << std::endl;
        // Try a smaller fallback resolution to succeed on systems with limited CMA
        try
        {
            // Ensure we clean up any partially-configured camera state
            try
            {
                camera.StopCamera();
            }
            catch (...) {}
            try
            {
                camera.CloseCamera();
            }
            catch (...) {}

            camera.GetOptions()->Set().width = 640;
            camera.GetOptions()->Set().height = 480;
            camera.OpenCamera();
            camera.ConfigureStill();
            camera.StartCamera();
            cameraRunning = true;
            stillConfigured = true;
            std::cerr << "Fallback to 640x480 succeeded." << std::endl;
        }
        catch (const std::exception &e2)
        {
            std::cerr << "Fallback failed: " << e2.what() << std::endl;
            std::cerr << "Either increase CMA memory in /boot/config.txt (cma=) or use a lower "
                         "resolution."
                      << std::endl;
            stillConfigured = false;
            return;
        }
    }

    stillConfigured = true;
}

void PiCam::ConfigureVideo(unsigned int width, unsigned int height, float framerate, int bitrate)
{
    // Use Set() to change the underlying OptsInternal members.
    encoder.GetOptions()->Set().width = width;
    encoder.GetOptions()->Set().height = height;
    encoder.GetOptions()->Set().framerate = static_cast<float>(framerate);
    encoder.GetOptions()->Set().bitrate.set(std::to_string(bitrate) + "bps");
    encoder.GetOptions()->Set().codec = "h264";

    videoConfigured = true;
}

void PiCam::CaptureStill(const std::string &outFile)
{
    if (!stillConfigured)
    {
        ConfigureStill(1920, 1080);
    }

    if (!cameraRunning)
    {
        // Defensive: ensure camera is running
        camera.OpenCamera();
        camera.ConfigureStill();
        camera.StartCamera();
        cameraRunning = true;
    }

    auto msg = camera.Wait();
    if (msg.type != RPiCamApp::MsgType::RequestComplete)
    {
        camera.StopCamera();
        camera.CloseCamera();
        throw std::runtime_error("Failed to capture still image.");
    }

    auto complete = std::get<CompletedRequestPtr>(msg.payload);

    RPiCamApp::Stream *imgStream = camera.StillStream(nullptr);
    if (!imgStream)
    {
        camera.StopCamera();
        camera.CloseCamera();
        throw std::runtime_error("Failed to get image stream.");
    }

    auto info = camera.GetStreamInfo(imgStream);

    BufferReadSync reader(&camera, complete->buffers[imgStream]);
    const auto &mem = reader.Get();

    // Debug: log plane counts and sizes to help diagnose empty/truncated JPEGs
    try
    {
        std::cerr << "CaptureStill: planes=" << mem.size() << "\n";
        for (size_t i = 0; i < mem.size(); ++i)
        {
            std::cerr << "  plane[" << i << "] size=" << mem[i].size() << "\n";
        }
    }
    catch (...)
    {
        std::cerr << "CaptureStill: failed to print plane sizes" << std::endl;
    }

    std::string outPath = "build/Assets/Images/" + outFile;
    try
    {
        jpeg_save(mem, info, complete->metadata, outPath, camera.CameraModel(),
                  static_cast<StillOptions const *>(camera.GetOptions()));
    }
    catch (const std::exception &e)
    {
        std::cerr << "jpeg_save threw: " << e.what() << std::endl;
    }
}

cv::Mat PiCam::CaptureToMat(bool saveDebugJPEG)
{
    if (!stillConfigured)
        ConfigureStill(1920, 1080);

    if (!cameraRunning)
    {
        camera.OpenCamera();
        // Let the pipeline select its processed format (usually YUV420/sYCC for this sensor)
        camera.ConfigureStill();
        camera.StartCamera();
        cameraRunning = true;
    }

    auto msg = camera.Wait();
    if (msg.type != RPiCamApp::MsgType::RequestComplete)
        throw std::runtime_error("Failed to capture still image.");

    auto complete = std::get<CompletedRequestPtr>(msg.payload);
    RPiCamApp::Stream *imgStream = camera.StillStream(nullptr);
    if (!imgStream)
        throw std::runtime_error("Failed to get image stream.");

    auto info = camera.GetStreamInfo(imgStream);
    BufferReadSync reader(&camera, complete->buffers[imgStream]);
    const auto &mem = reader.Get();

    if (mem.empty() || mem[0].size() == 0)
        throw std::runtime_error("Captured empty image buffer");

    // Quick win: if the camera returned a JPEG blob (common when encoding="jpg"),
    // decode it directly with OpenCV rather than attempting fragile YUV/Bayer reinterprets.
    // JPEG files start with 0xFF 0xD8.
    try {
        if (!mem.empty() && mem[0].size() >= 2 && mem[0][0] == static_cast<uint8_t>(0xFF) && mem[0][1] == static_cast<uint8_t>(0xD8)) {
            std::vector<uint8_t> jpegBuf(mem[0].begin(), mem[0].end());
            cv::Mat img = cv::imdecode(jpegBuf, cv::IMREAD_COLOR);
            if (!img.empty()) {
                if (saveDebugJPEG) cv::imwrite("build/Assets/Images/debug_from_jpeg.jpg", img);
                return img;
            } else {
                std::cerr << "CaptureToMat: imdecode failed for JPEG buffer\n";
            }
        }
    } catch (const std::exception &e) {
        std::cerr << "CaptureToMat: JPEG decode attempt threw: " << e.what() << "\n";
    }

    // Try to be robust to several possible pixel layouts. Prefer BGR if the stream
    // was requested with FLAG_STILL_BGR, but some pipelines may still return YUV.
    int w = info.width;
    int h = info.height;
    std::string fmt;
    try
    {
        fmt = info.pixel_format.toString();
    }
    catch (...)
    {
        fmt = "";
    }

    std::cerr << "CaptureToMat: pixel_format=" << fmt << " planes=" << mem.size() << "\n";
    bool gray_detected = false;

    // Case 1: already BGR888 in one plane
    if ((fmt.find("BGR") != std::string::npos || mem.size() == 1) &&
        mem[0].size() >= static_cast<size_t>(w * h * 3))
    {
        const int bpp = 3; // bytes per pixel for BGR888
        // info.stride may be reported in bytes or in pixels depending on pipeline. Compute a
        // conservative stride_bytes value (must be >= w*bpp and <= mem[0].size()/h)
        size_t stride_bytes =
            info.stride ? static_cast<size_t>(info.stride) : static_cast<size_t>(w * bpp);
        if (stride_bytes < static_cast<size_t>(w * bpp))
            stride_bytes = static_cast<size_t>(info.stride) * bpp; // assume stride was pixels
        // Cap stride to available buffer size per row
        size_t max_stride = mem[0].size() / static_cast<size_t>(h);
        if (stride_bytes > max_stride)
            stride_bytes = max_stride;

        std::cerr << "CaptureToMat: BGR path: w=" << w << " h=" << h
                  << " stride_bytes=" << stride_bytes << " mem0_size=" << mem[0].size() << "\n";

        cv::Mat img(h, w, CV_8UC3, const_cast<uint8_t *>(mem[0].data()), stride_bytes);

    // Quick diagnostics: compute mean/stddev per channel to detect identical channels (grey)
        try
        {
            std::vector<cv::Mat> ch;
            cv::split(img, ch);
            cv::Scalar m0, s0, m1, s1, m2, s2;
            if (ch.size() == 3)
            {
                cv::meanStdDev(ch[0], m0, s0);
                cv::meanStdDev(ch[1], m1, s1);
                cv::meanStdDev(ch[2], m2, s2);
                std::cerr << "  channel means: B=" << m0[0] << " G=" << m1[0] << " R=" << m2[0]
                          << " stddevs: B=" << s0[0] << " G=" << s1[0] << " R=" << s2[0] << "\n";

                double meanBG = std::abs(m0[0] - m1[0]);
                double meanBR = std::abs(m0[0] - m2[0]);
                double meanGR = std::abs(m1[0] - m2[0]);
                double avgStd = (s0[0] + s1[0] + s2[0]) / 3.0;
                // Heuristic: if means are nearly equal and overall stddev is low, treat as grey
                if (meanBG < 2.5 && meanBR < 2.5 && meanGR < 2.5 && avgStd < 6.0)
                    gray_detected = true;
            }
        }
        catch (...)
        {
        }

        cv::Mat out = img.clone();
        if (saveDebugJPEG)
            cv::imwrite("build/Assets/Images/debug.jpg", out);

        if (!gray_detected)
            return out;

        std::cerr
            << "CaptureToMat: detected near-gray BGR buffer; attempting YUV reinterpretation...\n";
    }

    // If we detected grayscale on a single-plane BGR buffer, attempt to reinterpret the
    // single plane as I420 or NV12 (some pipelines pack YUV into a single contiguous buffer).
    if (mem.size() == 1)
    {
        // Robust single-plane YUV reconstruction.
        // Many pipelines provide a single contiguous buffer with per-row padding. We
        // detect the most likely Y-row stride by minimizing inter-row difference, then
        // reconstruct compact Y, U, V (I420) or packed NV12 and convert.
        size_t total = mem[0].size();
        size_t minNeeded = static_cast<size_t>(w) * static_cast<size_t>(h);
        if (total < minNeeded)
        {
            std::cerr << "CaptureToMat: single-plane buffer too small (" << total << ")\n";
        }
        else
        {
            const uint8_t *src = mem[0].data();
            size_t minStride = static_cast<size_t>(w);
            size_t maxStride = std::min(static_cast<size_t>(w + 4096), total / static_cast<size_t>(h));
            if (maxStride < minStride) maxStride = minStride;

            auto score_stride = [&](size_t strideY)->double
            {
                int samples = 8;
                double acc = 0.0;
                int count = 0;
                for (int s = 0; s < samples; ++s)
                {
                    int r = (s * (h - 1)) / (samples - 1);
                    size_t offA = static_cast<size_t>(r) * strideY;
                    size_t offB = static_cast<size_t>(std::min(r + 1, h - 1)) * strideY;
                    if (offB + static_cast<size_t>(w) > total || offA + static_cast<size_t>(w) > total)
                        continue;
                    const uint8_t *a = src + offA;
                    const uint8_t *b = src + offB;
                    double sum = 0.0;
                    for (size_t i = 0; i < static_cast<size_t>(w); ++i)
                        sum += std::abs(static_cast<int>(a[i]) - static_cast<int>(b[i]));
                    acc += sum / static_cast<double>(w);
                    ++count;
                }
                return (count > 0) ? (acc / count) : 1e9;
            };

            size_t bestStride = minStride;
            double bestScore = 1e18;
            for (size_t stride = minStride; stride <= maxStride; ++stride)
            {
                double sc = score_stride(stride);
                if (sc < bestScore)
                {
                    bestScore = sc;
                    bestStride = stride;
                }
            }

            std::cerr << "CaptureToMat: single-plane detected strideY=" << bestStride
                      << " score=" << bestScore << " total=" << total << "\n";

            size_t offsetY = bestStride * static_cast<size_t>(h);
            if (offsetY < total)
            {
                size_t uvBytes = total - offsetY;
                size_t uvh = static_cast<size_t>(h) / 2;
                size_t strideUV = (uvh > 0) ? ((uvBytes + uvh - 1) / uvh) : 0;

                // Debug dump: print offsets, strides and first bytes of a few Y/UV rows to
                // help diagnose per-row padding/alignment issues that cause vertical lines.
                auto hexdump = [](const uint8_t *p, size_t len)->std::string {
                    std::ostringstream ss;
                    ss << std::hex << std::setfill('0');
                    for (size_t i = 0; i < len; ++i)
                    {
                        ss << std::setw(2) << (static_cast<int>(p[i]) & 0xff);
                        if (i + 1 < len) ss << ' ';
                    }
                    return ss.str();
                };

                std::cerr << "CaptureToMat: offsetY=" << offsetY << " uvBytes=" << uvBytes
                          << " uvh=" << uvh << " strideUV=" << strideUV << "\n";

                // sample some Y rows
                size_t sampleY[3] = {0, static_cast<size_t>(h/2), static_cast<size_t>(h-1)};
                for (size_t si = 0; si < 3; ++si)
                {
                    size_t r = sampleY[si];
                    if (r < static_cast<size_t>(h))
                    {
                        const uint8_t *p = src + r * bestStride;
                        size_t dumpLen = std::min(static_cast<size_t>(32), static_cast<size_t>(w));
                        std::cerr << "Y row " << r << " first " << dumpLen << " bytes: "
                                  << hexdump(p, dumpLen) << "\n";
                    }
                }

                // sample some UV rows (if present)
                if (uvh > 0 && strideUV > 0)
                {
                    size_t sampleUV[3] = {0, uvh/2, uvh > 0 ? uvh-1 : 0};
                    for (size_t si = 0; si < 3; ++si)
                    {
                        size_t r = sampleUV[si];
                        if (r < uvh)
                        {
                            const uint8_t *p = src + offsetY + r * strideUV;
                            size_t dumpLen = std::min(static_cast<size_t>(32), strideUV);
                            std::cerr << "UV row " << r << " first " << dumpLen << " bytes: "
                                      << hexdump(p, dumpLen) << "\n";
                        }
                    }
                }

                // Copy compact Y
                std::vector<uint8_t> Y; Y.resize(static_cast<size_t>(w) * static_cast<size_t>(h));
                for (int r = 0; r < h; ++r)
                {
                    const uint8_t *p = src + static_cast<size_t>(r) * bestStride;
                    memcpy(Y.data() + static_cast<size_t>(r) * static_cast<size_t>(w), p, static_cast<size_t>(w));
                }

                // Try I420 (U plane then V plane)
                size_t expectedUVPlane = (static_cast<size_t>(w) / 2) * uvh;
                if (uvBytes >= expectedUVPlane * 2 && strideUV > 0)
                {
                    std::vector<uint8_t> U; U.resize(expectedUVPlane);
                    std::vector<uint8_t> V; V.resize(expectedUVPlane);
                    const uint8_t *uvBase = src + offsetY;
                    bool ok = true;
                    for (size_t r = 0; r < uvh; ++r)
                    {
                        const uint8_t *pU = uvBase + r * strideUV;
                        const uint8_t *pV = uvBase + (uvh * strideUV) + r * strideUV;
                        if (static_cast<size_t>(pV - src) + static_cast<size_t>(w / 2) > total) { ok = false; break; }
                        memcpy(U.data() + r * (static_cast<size_t>(w) / 2), pU, static_cast<size_t>(w) / 2);
                        memcpy(V.data() + r * (static_cast<size_t>(w) / 2), pV, static_cast<size_t>(w) / 2);
                    }
                        if (ok)
                        {
                            auto compute_metrics = [](const cv::Mat &bgr)->std::pair<double,double>
                            {
                                std::vector<cv::Mat> ch; cv::split(bgr, ch);
                                cv::Scalar m0, s0, m1, s1, m2, s2;
                                cv::meanStdDev(ch[0], m0, s0);
                                cv::meanStdDev(ch[1], m1, s1);
                                cv::meanStdDev(ch[2], m2, s2);
                                double avgStd = (s0[0] + s1[0] + s2[0]) / 3.0;
                                double meanDiff = (std::abs(m0[0]-m1[0]) + std::abs(m0[0]-m2[0]) + std::abs(m1[0]-m2[0]))/3.0;
                                return {avgStd, meanDiff};
                            };

                            std::vector<uint8_t> packed; packed.reserve(Y.size() + U.size() + V.size());
                            packed.insert(packed.end(), Y.begin(), Y.end());
                            packed.insert(packed.end(), U.begin(), U.end());
                            packed.insert(packed.end(), V.begin(), V.end());
                            cv::Mat yuvMat(static_cast<int>(h + h / 2), w, CV_8UC1, packed.data());
                            cv::Mat bgr;
                            try { cv::cvtColor(yuvMat, bgr, cv::COLOR_YUV2BGR_I420); }
                            catch (...) { bgr.release(); }
                            if (!bgr.empty())
                            {
                                auto [avgStd, meanDiff] = compute_metrics(bgr);
                                std::cerr << "I420 fallback metrics: avgStd=" << avgStd << " meanDiff=" << meanDiff << "\n";
                                if (avgStd > 6.0 || meanDiff > 2.5)
                                {
                                    if (saveDebugJPEG) cv::imwrite("build/Assets/Images/debug_from_i420_fallback.jpg", bgr);
                                    return bgr.clone();
                                }
                                // try YV12 (swap U and V)
                                std::vector<uint8_t> packed_yv;
                                packed_yv.reserve(Y.size() + U.size() + V.size());
                                packed_yv.insert(packed_yv.end(), Y.begin(), Y.end());
                                packed_yv.insert(packed_yv.end(), V.begin(), V.end());
                                packed_yv.insert(packed_yv.end(), U.begin(), U.end());
                                cv::Mat yvMat(static_cast<int>(h + h / 2), w, CV_8UC1, packed_yv.data());
                                cv::Mat bgr_yv;
                                try { cv::cvtColor(yvMat, bgr_yv, cv::COLOR_YUV2BGR_YV12); }
                                catch (...) { bgr_yv.release(); }
                                if (!bgr_yv.empty())
                                {
                                    auto [a2, m2] = compute_metrics(bgr_yv);
                                    std::cerr << "YV12 fallback metrics: avgStd=" << a2 << " meanDiff=" << m2 << "\n";
                                    if (a2 > 6.0 || m2 > 2.5)
                                    {
                                        if (saveDebugJPEG) cv::imwrite("build/Assets/Images/debug_from_yv12_fallback.jpg", bgr_yv);
                                        return bgr_yv.clone();
                                    }
                                }
                            }
                        }
                }

                // Try NV12 (interleaved UV rows)
                if (uvBytes >= static_cast<size_t>(w) * uvh && strideUV > 0)
                {
                    std::vector<uint8_t> packed; packed.reserve(Y.size() + static_cast<size_t>(w) * uvh);
                    packed.insert(packed.end(), Y.begin(), Y.end());
                    const uint8_t *uvBase = src + offsetY;
                    for (size_t r = 0; r < uvh; ++r)
                    {
                        const uint8_t *p = uvBase + r * strideUV;
                        packed.insert(packed.end(), p, p + static_cast<size_t>(w));
                    }
                    cv::Mat yuvMat(static_cast<int>(h + h / 2), w, CV_8UC1, packed.data());
                    cv::Mat bgr;
                    try { cv::cvtColor(yuvMat, bgr, cv::COLOR_YUV2BGR_NV12); }
                    catch (...) { bgr.release(); }
                    if (!bgr.empty())
                    {
                        // compute metrics and try NV21 if NV12 looks wrong
                        auto compute_metrics = [](const cv::Mat &bgr)->std::pair<double,double>
                        {
                            std::vector<cv::Mat> ch; cv::split(bgr, ch);
                            cv::Scalar m0, s0, m1, s1, m2, s2;
                            cv::meanStdDev(ch[0], m0, s0);
                            cv::meanStdDev(ch[1], m1, s1);
                            cv::meanStdDev(ch[2], m2, s2);
                            double avgStd = (s0[0] + s1[0] + s2[0]) / 3.0;
                            double meanDiff = (std::abs(m0[0]-m1[0]) + std::abs(m0[0]-m2[0]) + std::abs(m1[0]-m2[0]))/3.0;
                            return {avgStd, meanDiff};
                        };
                        auto [avgStd, meanDiff] = compute_metrics(bgr);
                        std::cerr << "NV12 fallback metrics: avgStd=" << avgStd << " meanDiff=" << meanDiff << "\n";

                        // If metrics look reasonable accept this conversion. Otherwise try NV21.
                        if (avgStd > 6.0 || meanDiff > 2.5)
                        {
                            // small saturation boost if image still looks desaturated
                            cv::Mat final_bgr = bgr;
                            try
                            {
                                cv::Mat hsv; cv::cvtColor(final_bgr, hsv, cv::COLOR_BGR2HSV);
                                std::vector<cv::Mat> hsvc; cv::split(hsv, hsvc);
                                cv::Scalar meanS = cv::mean(hsvc[1]);
                                if (meanS[0] < 40.0)
                                {
                                    hsvc[1].convertTo(hsvc[1], -1, 1.6, 0); // increase saturation
                                    cv::merge(hsvc, hsv);
                                    cv::cvtColor(hsv, final_bgr, cv::COLOR_HSV2BGR);
                                    std::cerr << "NV12 fallback: applied saturation boost\n";
                                }
                            }
                            catch (...) { /* non-fatal */ }

                            if (saveDebugJPEG) cv::imwrite("build/Assets/Images/debug_from_nv12_fallback.jpg", final_bgr);
                            std::cerr << "CaptureToMat: chosen path=NV12\n";
                            return final_bgr.clone();
                        }

                        // try NV21: swap U/V pairs in the UV rows
                        cv::Mat bgr_nv21;
                        try
                        {
                            // create NV21 by swapping each pair in the UV rows
                            std::vector<uint8_t> packed_nv21; packed_nv21.reserve(packed.size());
                            packed_nv21.insert(packed_nv21.end(), Y.begin(), Y.end());
                            const uint8_t *uvBase = src + offsetY;
                            for (size_t r = 0; r < uvh; ++r)
                            {
                                const uint8_t *p = uvBase + r * strideUV;
                                for (size_t i = 0; i < static_cast<size_t>(w); i += 2)
                                {
                                    // original assumed UV order, swap to VU
                                    packed_nv21.push_back(p[i+1]);
                                    packed_nv21.push_back(p[i]);
                                }
                            }
                            cv::Mat yuv_nv21(static_cast<int>(h + h / 2), w, CV_8UC1, packed_nv21.data());
                            cv::cvtColor(yuv_nv21, bgr_nv21, cv::COLOR_YUV2BGR_NV21);
                        }
                        catch (...) { bgr_nv21.release(); }
                        if (!bgr_nv21.empty())
                        {
                            auto [a2, m2] = compute_metrics(bgr_nv21);
                            std::cerr << "NV21 fallback metrics: avgStd=" << a2 << " meanDiff=" << m2 << "\n";
                            if (a2 > 6.0 || m2 > 2.5)
                            {
                                if (saveDebugJPEG) cv::imwrite("build/Assets/Images/debug_from_nv21_fallback.jpg", bgr_nv21);
                                std::cerr << "CaptureToMat: chosen path=NV21\n";
                                return bgr_nv21.clone();
                            }
                        }
                    }
                }
            }
        }
    }

    // Case 2: I420 / YUV420 planar (3 planes: Y, U, V) -> convert using OpenCV
    if (mem.size() >= 3)
    {
        size_t expectedY = static_cast<size_t>(w) * static_cast<size_t>(h);
        size_t expectedUV = (static_cast<size_t>(w) / 2) * (static_cast<size_t>(h) / 2);
        if (mem[0].size() >= expectedY && mem[1].size() >= expectedUV &&
            mem[2].size() >= expectedUV)
        {
            std::vector<uint8_t> yuv;
            yuv.reserve(expectedY + expectedUV + expectedUV);
            yuv.insert(yuv.end(), mem[0].begin(), mem[0].begin() + expectedY);
            yuv.insert(yuv.end(), mem[1].begin(), mem[1].begin() + expectedUV);
            yuv.insert(yuv.end(), mem[2].begin(), mem[2].begin() + expectedUV);

            // Create a single-channel Mat of height + h/2 for I420 layout and convert
            cv::Mat yuvMat(static_cast<int>(h + h / 2), w, CV_8UC1, yuv.data());
            cv::Mat bgr;
            cv::cvtColor(yuvMat, bgr, cv::COLOR_YUV2BGR_I420);
            cv::Mat out = bgr.clone();
            if (saveDebugJPEG)
                cv::imwrite("build/Assets/Images/debug_from_i420.jpg", out);
            return out;
        }
    }

    // Case 3: NV12 (2 planes: Y, interleaved UV)
    if (mem.size() >= 2)
    {
        size_t expectedY = static_cast<size_t>(w) * static_cast<size_t>(h);
        size_t expectedUV = static_cast<size_t>(w) * static_cast<size_t>(h / 2);
        if (mem[0].size() >= expectedY && mem[1].size() >= expectedUV)
        {
            std::vector<uint8_t> yuv;
            yuv.reserve(expectedY + expectedUV);
            yuv.insert(yuv.end(), mem[0].begin(), mem[0].begin() + expectedY);
            yuv.insert(yuv.end(), mem[1].begin(), mem[1].begin() + expectedUV);

            cv::Mat yuvMat(static_cast<int>(h + h / 2), w, CV_8UC1, yuv.data());
            cv::Mat bgr;
            cv::cvtColor(yuvMat, bgr, cv::COLOR_YUV2BGR_NV12);
            cv::Mat out = bgr.clone();
            if (saveDebugJPEG)
                cv::imwrite("build/Assets/Images/debug_from_nv12.jpg", out);
            return out;
        }
    }

    // If none of the above worked, attempt a conservative row-wise copy from plane[0]
    try
    {
        size_t stride = info.stride ? info.stride : (w * 3);
        cv::Mat img(h, w, CV_8UC3);
        const uint8_t *src = mem[0].data();
        for (int r = 0; r < h; ++r)
        {
            const uint8_t *rowSrc = src + (size_t)r * stride;
            uint8_t *rowDst = img.ptr<uint8_t>(r);
            memcpy(rowDst, rowSrc, static_cast<size_t>(w * 3));
        }
        cv::Mat out = img.clone();
        if (saveDebugJPEG)
            cv::imwrite("build/Assets/Images/debug_from_guess.jpg", out);
        return out;
    }
    catch (const std::exception &e)
    {
        throw std::runtime_error(std::string("Unsupported/unknown capture format: ") + e.what());
    }
}

void PiCam::CaptureVideo(const std::string &outFile, int durationSeconds)
{
    if (!videoConfigured)
    {
        ConfigureVideo(1920, 1080);
    }

    // Open/Configure/Start
    encoder.OpenCamera();
    encoder.ConfigureVideo();
    encoder.StartCamera();

    // Hook to receive encoded output. encode callback receives (void* mem, size_t size, int64_ts,
    // bool eos)
    std::ofstream ofs("build/Assets/Videos/" + outFile, std::ios::binary);
    encoder.SetEncodeOutputReadyCallback(
        [&ofs](void *mem, size_t size, int64_t /*ts*/, bool /*eos*/)
        {
            if (mem && size)
                ofs.write(reinterpret_cast<char *>(mem), size);
        });

    // Create encoder internals and start encoding
    encoder.StartEncoder();

    // Run for durationSeconds, processing request-complete messages and asking the encoder
    auto start = std::chrono::steady_clock::now();
    while (
        std::chrono::duration_cast<std::chrono::seconds>(std::chrono::steady_clock::now() - start)
            .count() < durationSeconds)
    {
        auto msg = encoder.Wait();
        if (msg.type == RPiCamApp::MsgType::RequestComplete)
        {
            auto completed = std::get<CompletedRequestPtr>(msg.payload);
            // Get the video stream (RPiCamEncoder::VideoStream())
            RPiCamApp::Stream *video_stream = encoder.VideoStream(nullptr);
            if (video_stream)
            {
                // Ask encoder to encode this completed request (it will call your callback with
                // encoded bytes)
                bool started = encoder.EncodeBuffer(completed, video_stream);
                (void)started; // if false, the frame was skipped (e.g. sync not ready)
            }
        }
        else if (msg.type == RPiCamApp::MsgType::Quit)
        {
            break;
        }
    }

    // Stop encoder and camera
    encoder.StopEncoder();
    encoder.StopCamera();
    encoder.CloseCamera();
    ofs.close();
}

void PiCam::listCameras()
{
    libcamera::CameraManager cameraManager;
    cameraManager.start();
    auto cameraList = cameraManager.cameras();
    std::cout << "Available Cameras:\n";
    for (const auto &cam : cameraList)
    {
        std::cout << " - ID: " << cam->id() << ")\n";
    }
    cameraManager.stop();
}

PiCam::~PiCam()
{
    if (cameraRunning)
    {
        camera.StopCamera();
        camera.CloseCamera();
        cameraRunning = false;
    }
}

void PiCam::Shutdown()
{
    try
    {
        camera.StopCamera();
    }
    catch (...) {}
    try
    {
        camera.Teardown();
    }
    catch (...) {}
    try
    {
        camera.CloseCamera();
    }
    catch (...) {}
    cameraRunning = false;
}